<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Docutils 0.16: http://docutils.sourceforge.net/" />
<title>oneAPI Technical Advisory Board Meeting (TAB-AI) Meeting Notes</title>
<style type="text/css">

/*
:Author: David Goodger (goodger@python.org)
:Id: $Id: html4css1.css 7952 2016-07-26 18:15:59Z milde $
:Copyright: This stylesheet has been placed in the public domain.

Default cascading style sheet for the HTML output of Docutils.

See http://docutils.sf.net/docs/howto/html-stylesheets.html for how to
customize this style sheet.
*/

/* used to remove borders from tables and images */
.borderless, table.borderless td, table.borderless th {
  border: 0 }

table.borderless td, table.borderless th {
  /* Override padding for "table.docutils td" with "! important".
     The right padding separates the table cells. */
  padding: 0 0.5em 0 0 ! important }

.first {
  /* Override more specific margin styles with "! important". */
  margin-top: 0 ! important }

.last, .with-subtitle {
  margin-bottom: 0 ! important }

.hidden {
  display: none }

.subscript {
  vertical-align: sub;
  font-size: smaller }

.superscript {
  vertical-align: super;
  font-size: smaller }

a.toc-backref {
  text-decoration: none ;
  color: black }

blockquote.epigraph {
  margin: 2em 5em ; }

dl.docutils dd {
  margin-bottom: 0.5em }

object[type="image/svg+xml"], object[type="application/x-shockwave-flash"] {
  overflow: hidden;
}

/* Uncomment (and remove this text!) to get bold-faced definition list terms
dl.docutils dt {
  font-weight: bold }
*/

div.abstract {
  margin: 2em 5em }

div.abstract p.topic-title {
  font-weight: bold ;
  text-align: center }

div.admonition, div.attention, div.caution, div.danger, div.error,
div.hint, div.important, div.note, div.tip, div.warning {
  margin: 2em ;
  border: medium outset ;
  padding: 1em }

div.admonition p.admonition-title, div.hint p.admonition-title,
div.important p.admonition-title, div.note p.admonition-title,
div.tip p.admonition-title {
  font-weight: bold ;
  font-family: sans-serif }

div.attention p.admonition-title, div.caution p.admonition-title,
div.danger p.admonition-title, div.error p.admonition-title,
div.warning p.admonition-title, .code .error {
  color: red ;
  font-weight: bold ;
  font-family: sans-serif }

/* Uncomment (and remove this text!) to get reduced vertical space in
   compound paragraphs.
div.compound .compound-first, div.compound .compound-middle {
  margin-bottom: 0.5em }

div.compound .compound-last, div.compound .compound-middle {
  margin-top: 0.5em }
*/

div.dedication {
  margin: 2em 5em ;
  text-align: center ;
  font-style: italic }

div.dedication p.topic-title {
  font-weight: bold ;
  font-style: normal }

div.figure {
  margin-left: 2em ;
  margin-right: 2em }

div.footer, div.header {
  clear: both;
  font-size: smaller }

div.line-block {
  display: block ;
  margin-top: 1em ;
  margin-bottom: 1em }

div.line-block div.line-block {
  margin-top: 0 ;
  margin-bottom: 0 ;
  margin-left: 1.5em }

div.sidebar {
  margin: 0 0 0.5em 1em ;
  border: medium outset ;
  padding: 1em ;
  background-color: #ffffee ;
  width: 40% ;
  float: right ;
  clear: right }

div.sidebar p.rubric {
  font-family: sans-serif ;
  font-size: medium }

div.system-messages {
  margin: 5em }

div.system-messages h1 {
  color: red }

div.system-message {
  border: medium outset ;
  padding: 1em }

div.system-message p.system-message-title {
  color: red ;
  font-weight: bold }

div.topic {
  margin: 2em }

h1.section-subtitle, h2.section-subtitle, h3.section-subtitle,
h4.section-subtitle, h5.section-subtitle, h6.section-subtitle {
  margin-top: 0.4em }

h1.title {
  text-align: center }

h2.subtitle {
  text-align: center }

hr.docutils {
  width: 75% }

img.align-left, .figure.align-left, object.align-left, table.align-left {
  clear: left ;
  float: left ;
  margin-right: 1em }

img.align-right, .figure.align-right, object.align-right, table.align-right {
  clear: right ;
  float: right ;
  margin-left: 1em }

img.align-center, .figure.align-center, object.align-center {
  display: block;
  margin-left: auto;
  margin-right: auto;
}

table.align-center {
  margin-left: auto;
  margin-right: auto;
}

.align-left {
  text-align: left }

.align-center {
  clear: both ;
  text-align: center }

.align-right {
  text-align: right }

/* reset inner alignment in figures */
div.align-right {
  text-align: inherit }

/* div.align-center * { */
/*   text-align: left } */

.align-top    {
  vertical-align: top }

.align-middle {
  vertical-align: middle }

.align-bottom {
  vertical-align: bottom }

ol.simple, ul.simple {
  margin-bottom: 1em }

ol.arabic {
  list-style: decimal }

ol.loweralpha {
  list-style: lower-alpha }

ol.upperalpha {
  list-style: upper-alpha }

ol.lowerroman {
  list-style: lower-roman }

ol.upperroman {
  list-style: upper-roman }

p.attribution {
  text-align: right ;
  margin-left: 50% }

p.caption {
  font-style: italic }

p.credits {
  font-style: italic ;
  font-size: smaller }

p.label {
  white-space: nowrap }

p.rubric {
  font-weight: bold ;
  font-size: larger ;
  color: maroon ;
  text-align: center }

p.sidebar-title {
  font-family: sans-serif ;
  font-weight: bold ;
  font-size: larger }

p.sidebar-subtitle {
  font-family: sans-serif ;
  font-weight: bold }

p.topic-title {
  font-weight: bold }

pre.address {
  margin-bottom: 0 ;
  margin-top: 0 ;
  font: inherit }

pre.literal-block, pre.doctest-block, pre.math, pre.code {
  margin-left: 2em ;
  margin-right: 2em }

pre.code .ln { color: grey; } /* line numbers */
pre.code, code { background-color: #eeeeee }
pre.code .comment, code .comment { color: #5C6576 }
pre.code .keyword, code .keyword { color: #3B0D06; font-weight: bold }
pre.code .literal.string, code .literal.string { color: #0C5404 }
pre.code .name.builtin, code .name.builtin { color: #352B84 }
pre.code .deleted, code .deleted { background-color: #DEB0A1}
pre.code .inserted, code .inserted { background-color: #A3D289}

span.classifier {
  font-family: sans-serif ;
  font-style: oblique }

span.classifier-delimiter {
  font-family: sans-serif ;
  font-weight: bold }

span.interpreted {
  font-family: sans-serif }

span.option {
  white-space: nowrap }

span.pre {
  white-space: pre }

span.problematic {
  color: red }

span.section-subtitle {
  /* font-size relative to parent (h1..h6 element) */
  font-size: 80% }

table.citation {
  border-left: solid 1px gray;
  margin-left: 1px }

table.docinfo {
  margin: 2em 4em }

table.docutils {
  margin-top: 0.5em ;
  margin-bottom: 0.5em }

table.footnote {
  border-left: solid 1px black;
  margin-left: 1px }

table.docutils td, table.docutils th,
table.docinfo td, table.docinfo th {
  padding-left: 0.5em ;
  padding-right: 0.5em ;
  vertical-align: top }

table.docutils th.field-name, table.docinfo th.docinfo-name {
  font-weight: bold ;
  text-align: left ;
  white-space: nowrap ;
  padding-left: 0 }

/* "booktabs" style (no vertical lines) */
table.docutils.booktabs {
  border: 0px;
  border-top: 2px solid;
  border-bottom: 2px solid;
  border-collapse: collapse;
}
table.docutils.booktabs * {
  border: 0px;
}
table.docutils.booktabs th {
  border-bottom: thin solid;
  text-align: left;
}

h1 tt.docutils, h2 tt.docutils, h3 tt.docutils,
h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {
  font-size: 100% }

ul.auto-toc {
  list-style-type: none }

</style>
</head>
<body>
<div class="document" id="oneapi-technical-advisory-board-meeting-tab-ai-meeting-notes">
<h1 class="title">oneAPI Technical Advisory Board Meeting (TAB-AI) Meeting Notes</h1>

<div class="section" id="id1">
<h1>2021-08-10</h1>
<div class="section" id="agenda">
<h2>Agenda</h2>
<table border="1" class="docutils">
<colgroup>
<col width="53%" />
<col width="29%" />
<col width="18%" />
</colgroup>
<tbody valign="top">
<tr><td>Why oneAPI, DPC++ Kick-off</td>
<td>James Brodman, Intel</td>
<td>30 min</td>
</tr>
<tr><td>oneAPI Threading Building Blocks</td>
<td>Mike Voss, Intel</td>
<td>25 min</td>
</tr>
<tr><td>oneAPI Data Analytics Library (oneDAL)</td>
<td>Nikolay Petrov, Intel</td>
<td>25 min</td>
</tr>
<tr><td>Opens / Topics</td>
<td>All</td>
<td>10 min</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="attendees">
<h2>Attendees</h2>
<table border="1" class="docutils">
<colgroup>
<col width="52%" />
<col width="48%" />
</colgroup>
<tbody valign="top">
<tr><td>Andrew Richards, Codeplay</td>
<td>Andrey Nikolaev, Intel</td>
</tr>
<tr><td>Mehdi Goli, Codeplay</td>
<td>Sujoy Saraswati, Habana</td>
</tr>
<tr><td>Atsushi Ike, Fujitsu</td>
<td>Tong Gu, Intel</td>
</tr>
<tr><td>Kentaro Kawakami, Fujitsu</td>
<td>Meena Arunachalam, Intel</td>
</tr>
<tr><td>Penporn Koanatakool, Google</td>
<td>Alison Richards, Intel</td>
</tr>
<tr><td>Sheng Zha, Apache MxNet</td>
<td>James Brodman, Intel</td>
</tr>
<tr><td>Judy Fox, University of Virginia</td>
<td>Michael Voss, Intel</td>
</tr>
<tr><td>Jun Qian, Vast AI Tech</td>
<td>Ligang Tian, Intel</td>
</tr>
<tr><td>Andrew Chen, Vast AI Tech</td>
<td>Guy Tamir, Intel</td>
</tr>
<tr><td>Nikolay A Petrov, Intel</td>
<td>Jian Hui Li, Intel</td>
</tr>
<tr><td>Rahul Khanna, Intel</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="slides">
<h2>Slides</h2>
<p><a class="reference external" href="presentations/oneAPIandDataParallelC++forAITAB.pdf">DPC++</a></p>
<p><a class="reference external" href="presentations/AI_TAB_oneTBB_0821.pdf">oneDAL</a></p>
<p><a class="reference external" href="presentations/AI_TAB_oneDALML.pdf">oneTBB</a></p>
</div>
<div class="section" id="discussion">
<h2>Discussion</h2>
<p>Question: Is TBB a good fit for heterogeneous compute or only for CPU?</p>
<p>Answer: Our strategy has been to keep TBB on the host but to work well
alongside of offloading to an accelerator.  Thought about how to
integrate executors into TBB.  We may have our generic algos accept
executors.  There are ways we might expand TBB to accept executors
that offload to accelerators.  In general, though, TBB is the way we
do efficient threading on the host.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Question: Are there things SYCL could learn from TBB?</p>
<p>Answer: Could be additional hints given for optimizing performance
with subgroups that could happen.  TBB does have these controls that
could be tuned for performance (or you can use the default and you may
be fine with that).  Setting partitioners.  None of that is exposed
yet in SYCL.</p>
<p>There is not a good interface for expressing graphs yet in SYCL so
there may be a way to gain some learnings there.  In SYCL you have
implicit graphs…but not explicit graphs so this could be an area of
learning from TBB.  Benefit would be to do some optimization and reuse
offload of kernels.  Host offload - give it the chunk it once and then
que up the kernels in a more optimal fashion.  Need to have repeatable
graphs in SYCL.</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>Question: Is this result on CPU or GPU– Slide Scikit Learn for
training and inference</p>
<p>Answer: CPU</p>
<p>Comment: Judy Fox mentioned she is teaching a python course and this
will give a lot of exposrue to Python.  Scikit learn bench – you can
easily download this and try out different sizes and algos and play
with that; Check out Medium.com blog for data analytic software for
additional information.</p>
</div>
<div class="section" id="opens">
<h2>OPENS</h2>
<p>Discuss upcoming topics from the team – happy to have members present
or share topics.  A few ideas:</p>
<p>Andrew Richards, Codeplay: doing simple code and showing how it goes
through SYCL and oneAPI Stack.  They are showing how it ends up on the
HW.  Showing the flow.</p>
<p>Medhi Goli, Codeplay: SYCL integrated w/ Tensorflow – Eigen support
the SYCL standard / oneAPI and how it supports CUDA as well</p>
<p>Codeplay: Support oneAPI on Nvidia GPUs</p>
<p><a class="reference external" href="https://www.youtube.com/watch?v=h5GamIZDrhE&amp;list=PLg-UKERBljNxsCltpcXU_Haz9xQSCN_SB&amp;index=8">Intel Extension for scikit-learn on youTube</a></p>
</div>
</div>
<div class="section" id="id2">
<h1>2021-05-20</h1>
<p>Attendees:</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody valign="top">
<tr><td>Alison Richards, Intel</td>
<td>Mourad Gouicem, Intel</td>
</tr>
<tr><td>Sanjiv Shah, Intel</td>
<td>Daniel M Lavery, Intel</td>
</tr>
<tr><td>Dmitry Durnov, Intel</td>
<td>Shlomo Raikin, Intel Habana</td>
</tr>
<tr><td>JF Massol, SiPearl</td>
<td>Rodolfo G Esteves, Intel</td>
</tr>
<tr><td>Atsushi Ike, Fujitsu</td>
<td>Mehdi Goli, Codeplay</td>
</tr>
<tr><td>Rajeev K Nalawadi, Intel</td>
<td>Rahul Khanna, Intel</td>
</tr>
<tr><td>Wei Cui, Microsoft</td>
<td>Andrew Richards, Codeplay</td>
</tr>
<tr><td>Jian Hui Li, Intel</td>
<td>Sreenivasulu Rayanki, Intel</td>
</tr>
<tr><td>Kentaro Kawakami, Fujitsu</td>
<td>Krishna Bhuyan, Intel</td>
</tr>
<tr><td>Tim Harris, Microsoft</td>
<td>Romain Dolbeau, SiPearl</td>
</tr>
<tr><td>Ruyman Reyes, Codeplay</td>
<td>Guoliang, VastAI Tech</td>
</tr>
<tr><td>Guy Tamir, Intel</td>
<td>Jayaram Bobba, Intel Habana</td>
</tr>
<tr><td>Igor Lopatin, Intel</td>
<td>Andrew Chen, VastAI Tech</td>
</tr>
<tr><td>Penporn Koanantakool, Google</td>
<td>AG Ramesh, Intel</td>
</tr>
<tr><td>Emad Barsoum, Cerebras</td>
<td>Andrey Nikolaev, Intel</td>
</tr>
<tr><td>Zack S Waters, Intel</td>
<td>Guangming Tan, ICT CAS</td>
</tr>
<tr><td>En Shao, ICT CAS</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
<p>Welcome, Vision, oneAPI spec update - Sanjiv Shah, Intel</p>
<ul>
<li><p class="first"><a class="reference external" href="presentations/2021-05-20-oneapi-spec.pdf">Slides</a></p>
</li>
<li><p class="first">Question: How does oneAPI differ from ROCM?</p>
<p>oneAPI is across multiple HW platforms and CPU, GPU, FPGA and
Accelerators, where ROCM is for AMD HW.  One could port Level Zero
to run on ROCm</p>
</li>
</ul>
<p>Antares for SYCL - Wei Cui, Microsoft</p>
<ul class="simple">
<li><a class="reference external" href="presentations/Antares4SyCL.pdf">Slides</a></li>
</ul>
<p>TensorFlow and oneDNN in Partnership - Penporn Koanantakool, Google</p>
<ul>
<li><p class="first"><a class="reference external" href="presentations/2021-05-20-TF-and-onednn.pdf">Slides</a></p>
</li>
<li><p class="first">Question: Will XLA and Jit be supported?</p>
<p>Jit is already used inside oneDNN.  Currently XLA is not using oneDNN.</p>
</li>
</ul>
<p>Intel Extension for TensorFlow Demo - Jian Hui Li, Intel</p>
<ul>
<li><p class="first">Intel extension for TensorFlow (TEX) uses modular TensorFlow
interface to bring intel XPU to TensorFlow for AI workload
acceleration.  oneAPI complements modular TensorFlow to provide
modular software architecture and unifies the programming interface
for AI hardware.</p>
</li>
<li><p class="first">Questions:</p>
<ul>
<li><p class="first">Is the code private or can people access it?</p>
<p>It is private for now but will be public.  Right now it is
pre-release trial and early exploration.</p>
</li>
<li><p class="first">If the new HW support oneDNN, will the integration to Framework just work?</p>
<p>ITEX uses oneAPI components includes oneDNN, oneCCL, and DPC++.
If the HW only supports oneDNN only, then only the most
performance critical subgraph is offloaded to hardware.
Supporting DPC++ and oneCCL can offload the whole deep learning
graph to the physical device.</p>
</li>
<li><p class="first">What is the preferred way of adding new devices to TF?</p>
<p>Pluggable devices is the preferred way to plug in new devices to
TF.</p>
</li>
</ul>
</li>
</ul>
<p>Enable Deep Learning Frameworks at Scale - Dmitry Durnov, Intel</p>
<ul class="simple">
<li><a class="reference external" href="presentations/ai_tab_oneccl.pdf">Slides</a></li>
<li>Feedback: this is a theoretical concept; can we take the concepts
and model the exercises in a real world example with x number of
training exercises.</li>
</ul>
</div>
<div class="section" id="id3">
<h1>2021-02-11</h1>
<p>Attendees:</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<tbody valign="top">
<tr><td>Ben Ashbaugh, Intel</td>
<td>Jeff McVeigh, Intel</td>
</tr>
<tr><td>Krishna Bhuyan, Intel</td>
<td>Rajeev  Nalawadi, Intel</td>
</tr>
<tr><td>Jayaram Bobba, Habana</td>
<td>Nikolay Petrov,  Intel</td>
</tr>
<tr><td>Andrew Chen, Vastai Tech</td>
<td>Jun Qian, Vastai Tech</td>
</tr>
<tr><td>Robert Cohn, Intel</td>
<td>Shlomo Raikin, Habana</td>
</tr>
<tr><td>Neel Dhamdhere, Intel</td>
<td>AG Ramesh, Intel</td>
</tr>
<tr><td>Mehdi Goli, Codeplay</td>
<td>Sreenivasulu Rayanki, Intel</td>
</tr>
<tr><td>Tim Harris, Microsoft</td>
<td>Leif Reinert, AWS</td>
</tr>
<tr><td>Atsushi Ike, Fujitsu</td>
<td>Ruyman Reyes, Codeplay</td>
</tr>
<tr><td>Kentaro Kawakami, Fujitsu</td>
<td>Alison Richards, Intel</td>
</tr>
<tr><td>Rahul Khanna, Intel</td>
<td>Andrew Richards, Codeplay</td>
</tr>
<tr><td>Kazui Kimihiko, Fujitsu</td>
<td>Tatiana Shpeisman, Google</td>
</tr>
<tr><td>Penporn Koanantakool, Google</td>
<td>Shivani Sud, Intel</td>
</tr>
<tr><td>Guoling Li, Vastai Tech</td>
<td>Guy Tamir, Intel</td>
</tr>
<tr><td>Hui Li, Intel</td>
<td>Zack Waters, Intel</td>
</tr>
<tr><td>Jian Hui Li, Intel</td>
<td>Louis Zhang, Vastai Tech</td>
</tr>
<tr><td>Wei Li, Intel</td>
<td>&nbsp;</td>
</tr>
</tbody>
</table>
<div class="line-block">
<div class="line">oneAPI Welcome &amp; Introduction – Jeff McVeigh</div>
<div class="line">AI Machine Learning Accelerators – Wei Li :  <a class="reference external" href="presentations/AI-TAB-Feb-2021.pdf">Slides</a></div>
</div>
<p>oneDNN on ARM – Kentaro Kawakami : <a class="reference external" href="presentations/oneAPI_development_of_oneDNN_for_Armv8-A_SVE_20210210_v4.pdf">Slides</a></p>
<ul>
<li><p class="first">How can we use Mesh TF widely to larger user base?</p>
<p>Fujistu team is working on Pull request with Google Mesh TF.</p>
</li>
</ul>
<p>oneDNN Graph API – Jian Hui Li : <a class="reference external" href="presentations/oneDNNGraph-oneAPIAITAB.final.pdf">Slides</a></p>
<ul>
<li><p class="first">How easy is it to add Graph Optimizations to new HW Backends?</p>
<p>Backends can develop their own graph optimizations to generate the
best optimized code. The implementation of oneDNN Graph API contains
an API layer and targets specific backends.  API layer focuses on
standardizing the operation and graph structure, which is then pass
to backends for optimization.  oneDNN Graph tensor supports opaque
tensors which allow backends to use private layout across the
partitions. We are aware that there is extra integration complexity
for framework to adopt opaque layouts, so the opaque tensor design
considered ease of use.  For backends which target large partition,
it can use the opaque tensor internally and use the public tensor as
partition input and output.</p>
</li>
<li><p class="first">Can one use SYCL for custom operations in a graph?</p>
<p>oneDNN Graph defines a set of operations.  Intel extensions for
Frameworks have DPC++/SYCL implementation of framework operations
outside of oneDNN Graph.  If the device implements oneDNN Graph and
is DPC++/SYCL compatible, it gets the maximum benefit of reusing
oneDNN Graph based framework integration and Intel extensions.
Registering a custom op to oneDNN Graph is in the future plan but
not defined yet.</p>
</li>
<li><p class="first">Any integration plans to integrate with MLIR?  Is this orthogonal to MLIR or a higher level integration?</p>
<p>Yes.  MLIR is multi-level IR, and oneDNN Graph op is at the same
level as high level MLIR dialect. We intercept at high level MLIR
dialect. We plan to have the integration when TF moves to MLIR as
the main graph representation.</p>
</li>
</ul>
<p>Level Zero – Ben Ashbaugh : <a class="reference external" href="presentations/21ww07_AI_TAB_Level_Zero.pdf">Slides</a></p>
<ul>
<li><p class="first">How do you adapt to different processors?  VPU, GPU or larger
constructs than kernels? Can all processors can be abstracted?</p>
<p>Some examples of device flexibility are the different device
property queries:
<a class="reference external" href="https://spec.oneapi.com/level-zero/latest/core/api.html#device">https://spec.oneapi.com/level-zero/latest/core/api.html#device</a></p>
<p>The specific case described on the call were command lists, which
are groups of commands that can represent a larger task graph:
<a class="reference external" href="https://spec.oneapi.com/level-zero/latest/core/PROG.html#command-lists">https://spec.oneapi.com/level-zero/latest/core/PROG.html#command-lists</a></p>
<p>If we need specific features for some other processor type we can
either add it in a future version of the spec, or it can be added as
an extension:
<a class="reference external" href="https://spec.oneapi.com/level-zero/latest/core/EXT.html">https://spec.oneapi.com/level-zero/latest/core/EXT.html</a></p>
</li>
<li><p class="first">Can we capture the capability of L0 (Create software with ability to
query)?</p>
<p>Please see the link above to the different device property queries.</p>
</li>
<li><p class="first">Do we need a plug-in for OpenVINO?  Do we need to develop Level-0
API?</p>
<p>OpenVINO is powered by oneAPI and is part of oneAPI
ecosystem. Implementing oneAPI including Level-0 certainly help
integrating to OpenVINO in a modular way.</p>
</li>
<li><p class="first">Can oneDNN co-exist with Level Zero?  Or can CPU code generator
co-exist with Level0?</p>
<p>Yes, oneDNN can co-exist with Level Zero, and oneDNN is one of the
layers that can be built on top of Level Zero.  A CPU code generator
would use a different mechanism currently though, because Level Zero
is not currently implemented for CPU devices.  See note below:</p>
<p>oneDNN works on top of L0. Though L0 does not support CPU (so there
is no sycl::device that uses L0 as a backend).  Here is what oneDNN
does for each type of device/backend.</p>
</li>
</ul>
<pre class="literal-block">
DPC++ device ----- CPU device -------------------- CPU jitted code is executed through sycl host_task
              \--- GPU device ---- L0 backend  --- binary is wrapped in L0 module, then in sycl program, and run through SYCL RT
                               \-- OCL backend --- binary is wrapped in OCL kernel, then in sycl program, and run through SYCL RT
</pre>
<p>Brainstorm Discussion:  Intros, Requirements, Use Cases, Q&amp; A - All</p>
</div>
</div>
</body>
</html>
